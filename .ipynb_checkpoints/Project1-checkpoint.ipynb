{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601581c8-ae32-4e8f-8536-e35528c20725",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "Yu Zhang, Benjamin Chen, Yuhan Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ed760-efa7-4855-be0d-257b9627e583",
   "metadata": {},
   "source": [
    "**The following packages are used in this project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09c5697-730f-4686-b1c9-0a9884db5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9075e-e25d-4a4f-9583-c956384bd9a3",
   "metadata": {},
   "source": [
    "1. **Write a function that loads this file and returns the matrix X and vector y. How many malicious data points are there? What can you say about the sparsity of the data?  Do you think it makes sense to use one-hot-coding for some of the columns?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce8b77-729e-4dbb-9207-914f717897e7",
   "metadata": {},
   "source": [
    "we use the following code to loads the data file, returns the matrix X and vector y, and count the number of malicious data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e864a9-9675-4156-b7cc-ef38e061133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 14700 malicious data points in the total 29332 data points\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, file_path= r\".\\data\\data.csv\"):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.num = 0\n",
    "\n",
    "        self._read(file_path)\n",
    "\n",
    "    def _read(self, file_path):\n",
    "        ori_data = np.loadtxt(file_path, skiprows=1, delimiter=',', dtype=int)\n",
    "        self.X = ori_data[:,:-1]\n",
    "        self.y = ori_data[:,-1]\n",
    "        self.y[self.y == 0] = -1\n",
    "        self.num = self.X.shape[0]\n",
    "\n",
    "    def split(self, ratio = 0.7):\n",
    "        # the number of training data\n",
    "        train_num = int(self.num * ratio)\n",
    "\n",
    "        # shuffling the original data\n",
    "        rand_idx = np.arange(self.num)\n",
    "        np.random.shuffle(rand_idx)\n",
    "        X = self.X[rand_idx]\n",
    "        y = self.y[rand_idx]\n",
    "\n",
    "        # split the data set\n",
    "        X_train, X_test = X[:train_num], X[train_num:]\n",
    "        y_train, y_test = y[:train_num], y[train_num:]\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "mydata = DataLoader(r\".\\data\\data.csv\")\n",
    "print(f'there are {(mydata.y == 1).sum()} \\\n",
    "malicious data points in the total {mydata.num} data points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2b207-2fc5-436a-838f-0e52bec26a08",
   "metadata": {},
   "source": [
    "From summing the ones in the data, we get that there are 14700 malicious data points in the total 29332 data points.\n",
    "\n",
    "The dataset is highly sparse, as most feature values are zero.\n",
    "\n",
    "For one-hot encoding, it is reasonable to apply it when the features represent categorical data without intrinsic ordering. However, for binary features in our project, one-hot encoding may be redundant unless further categorization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f1ba3-1e1d-44fc-8168-5c48b7c4d83b",
   "metadata": {},
   "source": [
    "2. **Write a function that splits the data into a training and test set according to some fraction 0 < r < 1. Make sure to use randomization; that is, it should not be the case that the training set consists of the first data points and the test set of the remaining data points. Your function should return matrices X train and X test and vectors y train and y test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c9bf1-27db-4eaa-8cfb-187bb539247f",
   "metadata": {},
   "source": [
    "We used the `split` method from the `DataLoader` class shown in **Question 1** to divide the dataset according to the given ratio, where shuffling is implemented before spliting to avoid the case that the training set consists of the first data points and the test set of the remaining data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56327077-481d-4818-b82f-2e138e7244d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "X_train, X_test, y_train, y_test = mydata.split(ratio = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1db227-e366-4960-b340-22bf26c38b62",
   "metadata": {},
   "source": [
    "3. **Write a function that, given the matrix X, the vector y, and a weight vector w defining a hyperplane, returns the number of correctly classified points. Verify that the output makes sense for random weight vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012600c7-85f3-43b5-bbbc-1efdaa59ce29",
   "metadata": {},
   "source": [
    "We used a randomly generated weight vector `w_init` to define a hyperplane. The followng function is used to compute predictions and the number of correctly classified points for the given input matrix `X`, output `y`and weight vector `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07764b8-0b5d-4520-93db-009b9ead714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13970 data points in total 29332 samples are correct using random generated weights\n"
     ]
    }
   ],
   "source": [
    "def evaluator(X: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "    y_pred = X.dot(w)\n",
    "    y_pred[y_pred < 0] = -1\n",
    "    y_pred[y_pred >= 0] = 1\n",
    "    return (y_pred == y).sum()\n",
    "    \n",
    "np.random.seed(2000)\n",
    "w_init = np.random.uniform(low = -1, high = 1, size=(mydata.X.shape[1]))\n",
    "pred_corr_num = evaluator(mydata.X, mydata.y, w_init)\n",
    "print(f'{pred_corr_num} data points in total {mydata.num} samples are correct using random generated weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fc90f-0f2c-4db9-9b5b-9db817bc11e6",
   "metadata": {},
   "source": [
    "4. **Consider the cost function for logistic regression as defined in the lectures. Write down a symbolic formula for the gradient of this function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe7154-0e96-4b3b-bf69-219cb6c50911",
   "metadata": {},
   "source": [
    "The cost function $J(w)$ for logistic regression with regularization is defined as:\n",
    "\n",
    "$$\n",
    "J(w) = \\sum_{i=1}^{n} L(y_i x_i^T w) + \\frac{\\lambda}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "L(s) = \\log\\left(1 + e^{-s}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The gradient of the loss term for a single data point $x_i$ with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_w L(y_i x_i^T w) = -\\frac{y_i x_i}{1 + e^{y_i x_i^T w}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f85a4c-6e47-40f5-bd45-93047c63f85c",
   "metadata": {},
   "source": [
    "Therefore, the gradient of the total loss term is:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\nabla_w L(y_i x_i^T w) = - \\sum_{i=1}^{n} \\frac{y_i x_i}{1 + e^{y_i x_i^T w}}\n",
    "$$\n",
    "\n",
    "The gradient of the regularization term $\\frac{\\lambda}{2} \\| w \\|^2$ with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\left( \\frac{\\lambda}{2} \\|w\\|^2 \\right) = \\lambda w\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610d840-5bd3-4498-b93a-615a129ca620",
   "metadata": {},
   "source": [
    "Thus, the gradient of  $J(w)$ with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = -\\sum_{i=1}^{n} \\frac{y_i x_i}{1 + e^{y_i x_i^T w}} + \\lambda w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ce8a1-2709-4590-94b4-29fc41eebfc5",
   "metadata": {},
   "source": [
    "5.\n",
    "    - **Write a straightforward implementation for logistic regression using gradient descent with a fixed step size $\\alpha$. Your function should take as arguments the data matrix $X$ and data vector $y$, the step size $\\alpha$, the regularization constant $\\lambda$, and an integer $K$ indicating the number of gradient descent steps. The function should return a weight vector $w$.**\n",
    "    - **Experiment with the hyperparameters, using dense and sparse linear algebra, on random splits of training and test data sets. (If you know about writing allocation-free code, you can also experiment with this.)**\n",
    "    - **Given for instance a 50/50 split between test and training data, what is the best classification performance you can obtain on the test set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3c5ad-c541-4e3a-b09b-82d23654c08c",
   "metadata": {},
   "source": [
    "In assessing the execution time of different implementations of logistic regression, we set the step size $\\alpha = 0.01$, regularization constant $\\lambda = 0.01$, and the number of gradient descent steps $K = 100$ to test our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497670e0-3956-451f-b3f0-392071b1e2ac",
   "metadata": {},
   "source": [
    "We implement our logistic regression code in three different ways:\n",
    "\n",
    "1. The `logistic_regression_entrywise` method calculates the gradient for each data point individually in every iteration.\n",
    "\n",
    "2. The `logistic_regression_vectorize` method leverages vectorization and broadcasting to avoid explicitly calculating the gradient for each data point, as shown below:\n",
    "\n",
    "    - `s = y * (X @ w)`\n",
    "    - `z = y / (1 + np.exp(s))`\n",
    "    - `w_grad = - (X.T @ z) + lambda_ * w`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d273b-5ce8-4cbe-9558-f529c2c20b31",
   "metadata": {},
   "source": [
    "3. The `logistic_regression_sparse method` is further optimized by converting the sparse training data `X` to a CSR (Compressed Sparse Row) format and using sparse linear algebra to accelerate the computation of matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7d026-315e-4428-9f18-34ee50a042a7",
   "metadata": {},
   "source": [
    "The code implement above is shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102884b6-5ccd-4bf4-abb7-070ee5fed088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "    def __init__(self, X_train, y_train, w_init):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.w_init = w_init\n",
    "        self.w = None\n",
    "\n",
    "    def logistic_regression(self, K, alpha, lambda_reg, flag = 0):\n",
    "        if flag == 0:\n",
    "            self.logistic_regression_entrywise(K, alpha, lambda_reg)\n",
    "        if flag == 1:\n",
    "            self.logistic_regression_vectorize(K, alpha, lambda_reg)\n",
    "        if flag == 2:\n",
    "            self.logistic_regression_sparse(K, alpha, lambda_reg)\n",
    "\n",
    "    def logistic_regression_entrywise(self, K, alpha, lambda_reg):\n",
    "        self.w = self.w_init\n",
    "        for k in range(K):\n",
    "            w_grad = lambda_reg * self.w\n",
    "            for i in range(self.X_train.shape[0]):\n",
    "                # if k == 5:\n",
    "                #     print(i)\n",
    "                w_grad -=(self.y_train[i] * self.X_train[i])\\\n",
    "                /(1 + np.exp(self.y_train[i] * self.X_train[i].dot(self.w)))\n",
    "            self.w -= w_grad * alpha\n",
    "        return self.w\n",
    "\n",
    "    def logistic_regression_vectorize(self, K, alpha, lambda_reg):\n",
    "        self.w = self.w_init\n",
    "        for _ in range(K):\n",
    "            s = self.y_train[:,None] * self.X_train @ self.w\n",
    "            z = self.y_train / (1 + np.exp(s))\n",
    "            w_grad = - self.X_train.T @ z + lambda_reg * self.w\n",
    "            self.w -= w_grad * alpha\n",
    "        return self.w\n",
    "\n",
    "    def logistic_regression_sparse(self, K, alpha, lambda_reg):\n",
    "        self.w = self.w_init\n",
    "        self.X_train = csr_matrix(self.X_train)\n",
    "        for _ in range(K):\n",
    "            s_product = self.X_train @ self.w\n",
    "            s = self.y_train * s_product\n",
    "            z = self.y_train / (1 + np.exp(s))\n",
    "            w_grad = - self.X_train.T @ z + lambda_reg * self.w\n",
    "            self.w -= w_grad * alpha\n",
    "        return self.w\n",
    "\n",
    "    def predict(self, X_test, y_test):\n",
    "        w = self.w\n",
    "        y_true =  evaluator(X_test, y_test, w)\n",
    "        acc = y_true / y_test.shape[0]\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4ec830-69c2-4fa0-8d29-d565bc4fbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry-Wise Implement:\n",
      " Accuracy on the test set: 95.06%, the time cost is 13.622644662857056s\n",
      "\n",
      "Dense Implement:\n",
      "Accuracy on the test set: 95.19%, the time cost is 1.6525468826293945s\n",
      "\n",
      "Sparse Implement:\n",
      "Accuracy on the test set: 95.25%, the time cost is 0.21705889701843262s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticClassifier(X_train, y_train, w_init)\n",
    "\n",
    "start = time.time()\n",
    "log_reg.logistic_regression(K=100, alpha=0.01, lambda_reg=0.01, flag= 0)\n",
    "accuracy = log_reg.predict(X_test, y_test)\n",
    "end = time.time()\n",
    "print(f'Entry-Wise Implement:\\n Accuracy on the test set: {accuracy * 100:.2f}%, the time cost is {end-start}s\\n')\n",
    "\n",
    "start = time.time()\n",
    "log_reg.logistic_regression(K=100, alpha=0.01, lambda_reg=0.01, flag=1)\n",
    "accuracy = log_reg.predict(X_test, y_test)\n",
    "end = time.time()\n",
    "print(f'Dense Implement:\\nAccuracy on the test set: {accuracy * 100:.2f}%, the time cost is {end-start}s\\n')\n",
    "\n",
    "start = time.time()\n",
    "log_reg.logistic_regression(K=100, alpha=0.01, lambda_reg=0.01, flag=2)\n",
    "accuracy = log_reg.predict(X_test, y_test)\n",
    "end = time.time()\n",
    "print(f'Sparse Implement:\\nAccuracy on the test set: {accuracy * 100:.2f}%, the time cost is {end - start}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b95fa4-5f1a-4f69-9a4b-653fe4bfffa4",
   "metadata": {},
   "source": [
    "From the above experiment, we have：\n",
    "1. The three implements achieve similar accuracy with different runing times.\n",
    " \n",
    "2. The `logistic_regression_entrywise` method is quite slow for large datasets because of the iterative sample-by-sample gradient computation, taking approximately 14.15 seconds for 100 iteration.\n",
    "\n",
    "3. The `logistic_regression_vectorize` method reduces computation time to roughly 2.26 second for 100 iteration.\n",
    "\n",
    "4. The `logistic_regression_sparse` method is the fastest with a rapid runtime of 0.31 seconds for 100 iteration, making it ideal for large, sparse datasets by reducing unnecessary calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95f488-aca3-490b-9401-aa6e5a3ea122",
   "metadata": {},
   "source": [
    "After experimenting with different hyperparameters, the best classification performance achieved in $1000$ iterations on test data with $50/50$ split is $95.38\\%$, with $\\alpha = 0.001$ and $\\lambda = 0.0001$ among all tested hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73c9dce-38a9-4e22-a2d7-7c490052cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: alpha = 0.1, lambda = 0.1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11787\\AppData\\Local\\Temp\\ipykernel_3796\\3012149010.py:43: RuntimeWarning: overflow encountered in exp\n",
      "  z = self.y_train / (1 + np.exp(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_acc = 0.9537706259375426, best_alpha = 0.001, best_lambda_reg = 0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_alpha = None\n",
    "best_lambda_reg = None\n",
    "best_acc = 0\n",
    "for alpha in [0.1,0.03,0.01,0.003,0.001]:\n",
    "    for lambda_reg in [0.1,0.01,0.001,0.0001]:\n",
    "        print(f\"Progress: alpha = {alpha}, lambda = {lambda_reg}\", end='\\r')\n",
    "        log_reg.logistic_regression(K=1000, alpha=alpha, lambda_reg=lambda_reg, flag=2)\n",
    "        accuracy = log_reg.predict(X_test, y_test)\n",
    "        if accuracy >= best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_alpha = alpha\n",
    "            best_lambda_reg = lambda_reg\n",
    "\n",
    "print(f'best_acc = {best_acc}, best_alpha = {best_alpha}, best_lambda_reg = {best_lambda_reg}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aff6c6-3ff8-40f6-ae0d-c8d28d2fdb7c",
   "metadata": {},
   "source": [
    "6. Download the file data2.csv. This is the same as the previous data file, except that 2000 fake data points have been appended to the data set. Use the singular value transform as explained in the lecture to detect and remove most of these outliers without removing too many other data points. (For the singular value transform you can use a library function, you do not have to implement this yourself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63674a-00e2-46c5-ad3d-1a234a3b4c6a",
   "metadata": {},
   "source": [
    "We use the following function to whitening our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd326dc3-197e-42b8-bcca-c9a9917d3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_whitening(X, k):\n",
    "    # Centering\n",
    "    M = X - X.mean(axis=0)\n",
    "\n",
    "    # SVD Decomposition\n",
    "    U, S, _ = la.svds(M, k=k)\n",
    "\n",
    "    # Whitening\n",
    "    Xw = U[:,::-1] @ scipy.sparse.diags(S[::-1])\n",
    "    Xw = Xw / np.std(Xw, axis=0)[None,:]\n",
    "\n",
    "    return Xw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30aee3-fb93-4da1-aafc-d30ac3540be0",
   "metadata": {},
   "source": [
    "Then we compute the norm of whitenning data to detect the outliers in our data, Within the `noise_removal` function, we employed the `scipy.sparse.linalg.svds` method to compute the top k singular values and their corresponding singular vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880e94dd-d817-43fa-a402-6f8b3fb7d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_removal(X, k, percentile = 0.9, vis_flag = False):\n",
    "    # Whitening\n",
    "    Xw = data_whitening(X, k)\n",
    "\n",
    "    norm_list = np.array([np.linalg.norm(Xw[i, :]) for i in range(Xw.shape[0])])\n",
    "    outlier_mask = norm_list >= np.quantile(norm_list, percentile)\n",
    "\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5977e-8135-4f09-9f2b-a22e724a2cff",
   "metadata": {},
   "source": [
    "Let\n",
    "- TP (True positive) = number of fake samples be detected\n",
    "- FN (False Negative) = number of fake samples be identified as true data points\n",
    "- TN (True Negative) = number of true data points be correctly identified\n",
    "- FP (False Positive) = number of true data point be wrongly identified as fake samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691dfc1-210d-4bec-bc8b-d62c8edef8c8",
   "metadata": {},
   "source": [
    "We use Precision, Recall and F1-score to assess the performance of our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa5ffa4-8ec7-4750-b73b-ca65b19f3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_performance(outlier_mask, vis_flag = False):\n",
    "    confusion_matrix, (P, R), F1 = F1_score(outlier_mask)\n",
    "    acc = (confusion_matrix[0] + confusion_matrix[2]) /\\\n",
    "              (confusion_matrix[0] + confusion_matrix[1] + confusion_matrix[2] + confusion_matrix[3])\n",
    "    \n",
    "    if vis_flag:\n",
    "        visualize_confusion(*confusion_matrix)\n",
    "        print(f'Accuracy: {acc*100:.2f}%')\n",
    "        print(f\"Precision: {P*100:.2f}%\")\n",
    "        print(f\"Recall: {R*100:.2f}%\")\n",
    "        print(f\"F1-score: {F1:.4f}\")\n",
    "\n",
    "    return F1, (P, R), acc\n",
    "\n",
    "def F1_score(outlier_mask, outlier_num = 2000):\n",
    "    \"\"\"\n",
    "    The outlier is in the last #outlier_num rows.\n",
    "    \"\"\"\n",
    "\n",
    "    total_num = len(outlier_mask)\n",
    "\n",
    "    TP = (outlier_mask[-outlier_num:].sum())\n",
    "    FP = (outlier_mask[:-outlier_num].sum())\n",
    "\n",
    "    TN = total_num - outlier_num - FP\n",
    "    FN = outlier_num - TP\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "    return (TP, FP, TN, FN), (Precision, Recall), F1_score\n",
    "\n",
    "\n",
    "def visualize_confusion(TP, FP, TN, FN):\n",
    "    confusion_matrix = np.array([[TN, FP],\n",
    "                                 [FN, TP]])\n",
    "\n",
    "    categories = ['Normal', 'Outlier']\n",
    "\n",
    "    # plot confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# read data\n",
    "my_data = DataLoader(r\".\\data\\data2.csv\")\n",
    "X = my_data.X.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df2ad7-10dc-4e8c-bc6d-a479206df061",
   "metadata": {},
   "source": [
    "The threshold is set based on the percentile of the norms of the whitened data, and we test on different percentile $q$ ranging from $90\\%$ to $94\\%$ to find the value of $k$ that maximizes the F1-score.\n",
    "\n",
    "The results show that $k = 85$, $q = 93\\%$ has the maximal F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e08488-7c20-448a-8890-d1b50b23864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: q=94.0%, k=73\r"
     ]
    }
   ],
   "source": [
    "F1_list = []\n",
    "best_F1 = 0\n",
    "best_k = None\n",
    "best_q = None\n",
    "\n",
    "for q in np.arange(0.90, 0.95, 0.01):  # test on different values of q\n",
    "    F1_values = []\n",
    "    for k in np.arange(1, 86):  # test on different values of k\n",
    "        print(f\"Progress: q={q * 100:.1f}%, k={k}\", end='\\r')\n",
    "        F1, _, _ = assess_performance(noise_removal(X, k, q), vis_flag=False)\n",
    "        F1_values.append(F1)\n",
    "\n",
    "        # Update best results if current F1 is higher\n",
    "        if F1 >= best_F1:\n",
    "            best_F1 = F1\n",
    "            best_k = k\n",
    "            best_q = q\n",
    "    \n",
    "    F1_list.append(F1_values)\n",
    "\n",
    "print(f\"Best performance: k={best_k} and q={best_q * 100:.0f}% achieve the maximal F1-score of {best_F1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb2512-d62a-4909-963b-f803bd41dc35",
   "metadata": {},
   "source": [
    "The changes of F1-score when $q = 93\\%$ with the increasing of $k$ is shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e273d-9ab2-47da-b4ee-abd3de7e5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_values = F1_list[3]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, 86), F1_values, marker='o')\n",
    "plt.xlabel(\"value of k\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"Variation in F1-score with Increasing of k\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dc75f-f257-4208-9b2b-9d35ff3f0d65",
   "metadata": {},
   "source": [
    "The detailed classification result for $q=93\\%$ and $k=85$ is shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38cbd3-47c8-4ff3-9155-0943bf79fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data whitening & noise removal\n",
    "F1, _, _ = assess_performance(noise_removal(X, 85, 0.93), vis_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd9f15-36d4-4489-b207-aecdeb95d95f",
   "metadata": {},
   "source": [
    "In the plot of confusion matrix, we found that:\n",
    "- The diagonal elements ($28974$ normal points and $1836$ outliers) indicate correctly classified instances.\n",
    "    \n",
    "- Off-diagonal elements ($164$ outliers misclassified as normal and $358$ normal samples misclassified as outliers) are the classification errors made by our model.\n",
    "\n",
    "- Majority of outliers was removed and only a few truer data point was misclassifieds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365e2a-fd7b-4317-86e2-038f5a6dab63",
   "metadata": {},
   "source": [
    "By adjusting the threshold $q$ for classifying a data point as an outlier or not, we obtain different classification results, focusing either on detecting more true positives (TP) or on detecting true positives with greater accuracy. The precision-recall curve is shown below：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3853d4-f12c-4e84-8206-4c9a85a7fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PRC(P_list, R_list):\n",
    "    # data clean\n",
    "    valid_indices = (~np.isnan(P_list)) & (~np.isnan(R_list)) & (~np.isinf(P_list)) & (~np.isinf(R_list))\n",
    "    P_array = P_list[valid_indices]\n",
    "    R_array = R_list[valid_indices]\n",
    "\n",
    "    # sort data\n",
    "    sorted_indices = np.argsort(R_array)\n",
    "    R_sorted = R_array[sorted_indices]\n",
    "    P_sorted = P_array[sorted_indices]\n",
    "\n",
    "    # plot data\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(R_sorted, P_sorted, linestyle='-', color='b')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "P_list = []\n",
    "R_list = []\n",
    "for q in np.arange(0.01, 1.00, 0.01):\n",
    "    print(f\"Progress: {q * 100:.0f}%\", end='\\r')\n",
    "    _, (P, R), _ = assess_performance(noise_removal(X, 85, q), vis_flag=False)\n",
    "    P_list.append(P)\n",
    "    R_list.append(R)\n",
    "\n",
    "# plot PR-Curve\n",
    "print(\"k = 85:\\n\")\n",
    "plot_PRC(np.array(P_list), np.array(R_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536f2ba-d982-4433-a0a9-cada702af1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
